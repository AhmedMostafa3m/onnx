{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('./best.pt')\n",
    "model.export(format='tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0477a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import onnx2tf\n",
    "\n",
    "model = YOLO('./best.pt')\n",
    "model.export(format='onnx',opset=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde97860",
   "metadata": {},
   "source": [
    "### from ONNX to TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431117af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"./best.onnx\")\n",
    "\n",
    "# Convert ONNX model to TensorFlow\n",
    "tf_rep = prepare(onnx_model)\n",
    "\n",
    "# Export the TensorFlow model to a temporary directory\n",
    "tf_rep.export_graph(\"./temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09930c01",
   "metadata": {},
   "source": [
    "default settings without optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76030ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_model_path_directory = './temp/'\n",
    "tflite_model_path = './tf_lite_model.tflite'\n",
    "\n",
    "# Convert the TensorFlow model to TFLite with TF Select ops enabled\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path_directory)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ONNX model has been successfully converted to TFLite and saved at {tflite_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c9ab6",
   "metadata": {},
   "source": [
    "### Optimized tf_lite version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ae52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf_model_path = './temp/'\n",
    "tflite_model_path = './tf_lite_model_optimized.tflite'\n",
    "# Convert the TensorFlow model to TFLite with TF Select ops enabled\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # Enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "# Apply optimization for quantization\n",
    "\"there are different optimizations available, we're just using the default optimization\"\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"ONNX model has been successfully converted to TFLite and saved at {tflite_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fa726",
   "metadata": {},
   "source": [
    "### Inference for quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ed69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tf_lite_model_optimized.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86374d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "input_data = np.random.randn(1,3,288,288).astype(np.float32)\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Process output data\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07013826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_data.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
